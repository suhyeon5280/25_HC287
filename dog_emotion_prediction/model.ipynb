{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":4969612,"sourceType":"datasetVersion","datasetId":2882322}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ============================\n# All-in-one Training (ImageFolder only, Kaggle)\n# ============================\n\nimport os, json, random, math\nfrom pathlib import Path\nfrom typing import List, Tuple\nimport numpy as np\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader, Subset\nfrom torchvision import datasets, transforms, models\n\n# ---- Device\nos.environ[\"CUDA_DEVICE_ORDER\"]=\"PCI_BUS_ID\"\nos.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\"\ndevice = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\ntorch.backends.cudnn.benchmark = True\nprint(\"Using device:\", device)\n\n# ---- Paths & Hyperparams\nDATA_ROOT = \"/kaggle/input/dog-emotion/Dog Emotion\"    # <- angry/happy/relaxed/sad 폴더가 바로 아래에\nOUT_DIR   = \"/kaggle/working\"\nIMG_SIZE  = 224\nBATCH     = 32\nVAL_RATIO = 0.2\nEPOCHS_PROBE = 3\nEPOCHS_MAIN  = 20\nLR_PROBE = 1e-3\nLR_MAIN  = 3e-4\nWD_MAIN  = 0.05\nLAMBDA_PRIOR = 0.5   # (지금은 prior 사용 안 함; 추후 on 가능)\n\n# ---- Utils\ndef set_seed(seed=42):\n    random.seed(seed); np.random.seed(seed)\n    torch.manual_seed(seed); torch.cuda.manual_seed_all(seed)\nset_seed(42)\n\ndef accuracy_top1(logits, targets):\n    return (logits.argmax(-1) == targets).float().mean().item()\n\ndef macro_f1(logits, targets, K):\n    pred = logits.argmax(-1)\n    f1=0.0\n    for c in range(K):\n        tp = ((pred==c)&(targets==c)).sum().item()\n        fp = ((pred==c)&(targets!=c)).sum().item()\n        fn = ((pred!=c)&(targets==c)).sum().item()\n        p = tp/(tp+fp+1e-8); r = tp/(tp+fn+1e-8)\n        f1 += 0.0 if (p+r)==0 else 2*p*r/(p+r)\n    return f1/K\n\ndef stratified_split(dataset, val_ratio=0.2, seed=42) -> Tuple[List[int], List[int]]:\n    labels = [s[1] for s in dataset.samples]\n    byc={}\n    for i,y in enumerate(labels): byc.setdefault(y,[]).append(i)\n    tr, va = [], []\n    for y,lst in byc.items():\n        r = random.Random(seed+y); r.shuffle(lst)\n        n_val = max(1, int(len(lst)*val_ratio))\n        va += lst[:n_val]; tr += lst[n_val:]\n    return tr, va\n\nclass FocalLoss(nn.Module):\n    def __init__(self, gamma=2.0): super().__init__(); self.gamma=gamma\n    def forward(self, logits, targets):\n        logp = F.log_softmax(logits, dim=-1); p = logp.exp()\n        tgt = F.one_hot(targets, num_classes=logits.size(-1)).float()\n        pt = (p*tgt).sum(-1)\n        return (-(1-pt)**self.gamma * (tgt*logp).sum(-1)).mean()\n\n# ---- Backbone (ResNet50 ImageNet 가중치)\nclass DinoResNet50Backbone(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.resnet = models.resnet50(weights=models.ResNet50_Weights.IMAGENET1K_V2)\n        self.resnet.fc = nn.Identity()\n    @property\n    def out_channels(self): return 2048\n    def forward(self, x):\n        x = self.resnet.conv1(x); x=self.resnet.bn1(x); x=self.resnet.relu(x); x=self.resnet.maxpool(x)\n        x = self.resnet.layer1(x); x=self.resnet.layer2(x); x=self.resnet.layer3(x)\n        feat = self.resnet.layer4(x)                      # (B,2048,7,7)\n        pooled = F.adaptive_avg_pool2d(feat,1).flatten(1) # (B,2048)\n        return feat, pooled\n\n# ---- Prior-Guided Transformer Head (prior는 기본 off)\ndef build_2d_sincos_pos_embed(h,w,dim,device):\n    gy, gx = torch.meshgrid(torch.arange(h,device=device), torch.arange(w,device=device), indexing=\"ij\")\n    omega = 1.0 / (10000 ** (torch.arange(dim//4, device=device).float()/(dim//4)))\n    oy = torch.einsum('hw,d->hwd', gy.float(), omega)\n    ox = torch.einsum('hw,d->hwd', gx.float(), omega)\n    pos = torch.cat([torch.sin(oy), torch.cos(oy), torch.sin(ox), torch.cos(ox)], dim=-1)\n    return pos.view(h*w, dim)\n\nclass PriorGuidedMHA(nn.Module):\n    def __init__(self, d_model=512, n_heads=8, attn_drop=0.0, proj_drop=0.0):\n        super().__init__(); assert d_model % n_heads==0\n        self.h=n_heads; self.d=d_model//n_heads; self.scale=self.d**-0.5\n        self.qkv = nn.Linear(d_model, d_model*3); self.proj = nn.Linear(d_model, d_model)\n        self.ad=nn.Dropout(attn_drop); self.pd=nn.Dropout(proj_drop)\n    def forward(self, x, prior_s=None, lam=0.0, return_attn=False):\n        B,N,C=x.shape\n        qkv=self.qkv(x).reshape(B,N,3,self.h,self.d).permute(2,0,3,1,4)\n        q,k,v=qkv[0]*self.scale, qkv[1], qkv[2]\n        logits = q @ k.transpose(-1,-2)\n        if prior_s is not None and lam>0:\n            s = torch.clamp(prior_s,0,1); bias = 0.5*(s.unsqueeze(2)+s.unsqueeze(1))\n            logits = logits + lam*bias.unsqueeze(1)\n        attn = F.softmax(logits, dim=-1); attn=self.ad(attn)\n        out = (attn@v).transpose(1,2).reshape(B,N,C)\n        out = self.pd(self.proj(out))\n        return (out, attn) if return_attn else out\n\nclass Block(nn.Module):\n    def __init__(self, d=512,h=8,mlp=4,drop=0.1):\n        super().__init__()\n        self.n1=nn.LayerNorm(d); self.attn=PriorGuidedMHA(d,h,proj_drop=drop)\n        self.n2=nn.LayerNorm(d)\n        self.ff=nn.Sequential(nn.Linear(d,int(d*mlp)), nn.GELU(), nn.Dropout(drop), nn.Linear(int(d*mlp),d))\n    def forward(self, x, prior_s, lam, return_attn=False):\n        y,a = self.attn(self.n1(x), prior_s, lam, return_attn=True); x=x+y\n        z = self.ff(self.n2(x)); x=x+z\n        return (x,a) if return_attn else x\n\nclass HeadConfig:\n    def __init__(self, num_classes=4, d_model=512, n_heads=8, n_layers=3, lambda_prior=0.5, drop=0.1, use_class_token=True):\n        self.num_classes=num_classes; self.d_model=d_model\n        self.n_heads=n_heads; self.n_layers=n_layers\n        self.lambda_prior=lambda_prior; self.drop=drop; self.use_class_token=use_class_token\n\nclass PriorGuidedEmotionModel(nn.Module):\n    def __init__(self, backbone=None, cfg:HeadConfig=HeadConfig()):\n        super().__init__()\n        self.backbone = backbone or DinoResNet50Backbone()\n        self.cfg=cfg; C=self.backbone.out_channels\n        self.proj = nn.Linear(C, cfg.d_model)\n        self.cls_token = nn.Parameter(torch.zeros(1,1,cfg.d_model)) if cfg.use_class_token else None\n        self.blocks = nn.ModuleList([Block(cfg.d_model, cfg.n_heads, drop=cfg.drop) for _ in range(cfg.n_layers)])\n        self.norm = nn.LayerNorm(cfg.d_model)\n        self.neck = nn.Sequential(nn.Linear(cfg.d_model,cfg.d_model), nn.GELU(), nn.Dropout(cfg.drop))\n        self.head_cls = nn.Linear(cfg.d_model, cfg.num_classes)\n        self.head_val = nn.Linear(cfg.d_model, 1)\n        self.head_ar  = nn.Linear(cfg.d_model, 1)\n    def forward(self, images, priors=None, return_attn=False):\n        feat,_ = self.backbone(images)                # (B,2048,7,7)\n        B,C,H,W = feat.shape\n        tok = self.proj(feat.permute(0,2,3,1).reshape(B,H*W,C))   # (B,49,D)\n        pos = build_2d_sincos_pos_embed(H,W,self.cfg.d_model, images.device).unsqueeze(0).expand(B,-1,-1)\n        x = tok + pos\n        if self.cfg.use_class_token:\n            cls = self.cls_token.expand(B,-1,-1); x = torch.cat([cls,x], dim=1)  # (B,50,D)\n            s = torch.zeros(B,1,device=images.device)\n            if priors is not None: s = torch.cat([s, priors.flatten(1)], dim=1)\n        else:\n            s = priors.flatten(1) if priors is not None else torch.zeros(B,x.size(1),device=images.device)\n        for blk in self.blocks:\n            x = blk(x, s, self.cfg.lambda_prior, return_attn=False)\n        x = self.norm(x)\n        f = x[:,0,:] if self.cfg.use_class_token else x.mean(1)\n        f = self.neck(f)\n        logits = self.head_cls(f)\n        val = torch.sigmoid(self.head_val(f)); ar=torch.sigmoid(self.head_ar(f))\n        return logits, val, ar, {}\n\n# ---- Transforms & Datasets\nnormalize = transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225])\ntrain_tf = transforms.Compose([\n    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.6,1.0)),\n    transforms.RandomHorizontalFlip(),\n    transforms.ColorJitter(0.2,0.2,0.2,0.05),\n    transforms.ToTensor(), normalize,\n])\nval_tf = transforms.Compose([\n    transforms.Resize(int(IMG_SIZE*1.15)),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor(), normalize,\n])\n\nfull_for_split = datasets.ImageFolder(DATA_ROOT, transform=val_tf)\nclasses = full_for_split.classes; num_classes=len(classes)\nprint(\"Classes:\", classes)  # ['angry','happy','relaxed','sad'] 여야 정상\n\ntrain_idx, val_idx = stratified_split(full_for_split, VAL_RATIO, seed=42)\ntrain_ds = Subset(datasets.ImageFolder(DATA_ROOT, transform=train_tf), train_idx)\nval_ds   = Subset(datasets.ImageFolder(DATA_ROOT, transform=val_tf),   val_idx)\n\ntrain_loader = DataLoader(train_ds, batch_size=BATCH, shuffle=True,  num_workers=2, pin_memory=True)\nval_loader   = DataLoader(val_ds,   batch_size=BATCH, shuffle=False, num_workers=2, pin_memory=True)\n\n# ---- Stage 1: Linear Probe\nbackbone = DinoResNet50Backbone().to(device)\nfor p in backbone.parameters(): p.requires_grad = False\nprobe = nn.Linear(backbone.out_channels, num_classes).to(device)\nopt_probe = torch.optim.AdamW(probe.parameters(), lr=LR_PROBE, weight_decay=0.01)\nce = nn.CrossEntropyLoss()\n\nprint(\"\\n== Stage 1: Linear Probe ==\")\nfor ep in range(1, EPOCHS_PROBE+1):\n    backbone.eval(); probe.train()\n    t_loss=t_acc=0\n    for imgs, labels in train_loader:\n        imgs,labels = imgs.to(device), labels.to(device)\n        with torch.no_grad(): _, pooled = backbone(imgs)\n        logits = probe(pooled)\n        loss = ce(logits, labels)\n        opt_probe.zero_grad(); loss.backward(); opt_probe.step()\n        t_loss += loss.item()*imgs.size(0)\n        t_acc  += (logits.argmax(-1)==labels).float().sum().item()\n    print(f\"[Probe] {ep}/{EPOCHS_PROBE} loss {t_loss/len(train_loader.dataset):.4f} acc {t_acc/len(train_loader.dataset):.4f}\")\n\n# ---- Stage 2: Main Training (prior OFF)\ncfg = HeadConfig(num_classes=num_classes, d_model=512, n_heads=8, n_layers=3, lambda_prior=LAMBDA_PRIOR, drop=0.1)\nmodel = PriorGuidedEmotionModel(backbone=backbone, cfg=cfg).to(device)\n\nopt_main = torch.optim.AdamW(model.parameters(), lr=LR_MAIN, weight_decay=WD_MAIN)\nfocal = FocalLoss(2.0)\n\nprint(\"\\n== Stage 2: Main Training ==\")\nbest_f1=-1.0; Path(OUT_DIR).mkdir(parents=True, exist_ok=True)\n\nfor ep in range(1, EPOCHS_MAIN+1):\n    model.train(); t_loss=t_acc=t_f1=0\n    for imgs, labels in train_loader:\n        imgs,labels = imgs.to(device), labels.to(device)\n        logits, val, ar, _ = model(imgs, priors=None, return_attn=False)\n        loss = focal(logits, labels)\n        opt_main.zero_grad(); loss.backward(); opt_main.step()\n        t_loss += loss.item()*imgs.size(0)\n        t_acc  += (logits.argmax(-1)==labels).float().sum().item()\n        t_f1   += macro_f1(logits, labels, num_classes)*imgs.size(0)\n\n    # val\n    model.eval(); v_loss=v_acc=v_f1=0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs,labels = imgs.to(device), labels.to(device)\n            logits, val, ar, _ = model(imgs, priors=None, return_attn=False)\n            loss = focal(logits, labels)\n            v_loss += loss.item()*imgs.size(0)\n            v_acc  += (logits.argmax(-1)==labels).float().sum().item()\n            v_f1   += macro_f1(logits, labels, num_classes)*imgs.size(0)\n\n    Ntr=len(train_loader.dataset); Nval=len(val_loader.dataset)\n    print(f\"[Main] {ep}/{EPOCHS_MAIN} | train loss {t_loss/Ntr:.4f} acc {t_acc/Ntr:.4f} f1 {t_f1/Ntr:.4f} \"\n          f\"| val loss {v_loss/Nval:.4f} acc {v_acc/Nval:.4f} f1 {v_f1/Nval:.4f}\")\n\n    if (v_f1/Nval) > best_f1:\n        best_f1 = v_f1/Nval\n        torch.save({\"model\":model.state_dict(),\"cfg\":cfg.__dict__,\"classes\":classes}, f\"{OUT_DIR}/best_model.pt\")\n\nprint(\"Done. Saved:\", f\"{OUT_DIR}/best_model.pt\")\nwith open(f\"{OUT_DIR}/classes.json\",\"w\") as f: json.dump({\"classes\":classes}, f, indent=2)\nprint(\"Classes saved to:\", f\"{OUT_DIR}/classes.json\")\n\n# ---- (Optional) Evaluation: Confusion matrix & per-class metrics\nimport numpy as np\nfrom collections import defaultdict\nckpt = torch.load(f\"{OUT_DIR}/best_model.pt\", map_permutation=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel.load_state_dict(ckpt[\"model\"]); model.eval()\n\ncm = np.zeros((num_classes, num_classes), dtype=int)\nper_cls = defaultdict(lambda: {\"tp\":0,\"fp\":0,\"fn\":0})\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits, _, _, _ = model(imgs, priors=None, return_attn=False)\n        preds = logits.argmax(-1)\n        for t,p in zip(labels.tolist(), preds.tolist()):\n            cm[t,p]+=1\n            if t==p: per_cls[t][\"tp\"]+=1\n            else: per_cls[p][\"fp\"]+=1; per_cls[t][\"fn\"]+=1\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\\n\", cm)\nfor c,name in enumerate(classes):\n    tp,fp,fn = per_cls[c][\"tp\"], per_cls[c][\"fp\"], per_cls[c][\"fn\"]\n    prec = tp/(tp+fp+1e-8); rec = tp/(tp+fn+1e-8)\n    f1 = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n    print(f\"{name:>8s} | P {prec:.3f} R {rec:.3f} F1 {f1:.3f}\")\n\n# ---- (Optional) ONNX export (prior 없이)\n# dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\n# torch.onnx.export(\n#     model, (dummy, None),\n#     f\"{OUT_DIR}/dog_emotion.onnx\",\n#     input_names=[\"image\",\"prior\"], output_names=[\"logits\",\"val\",\"ar\"],\n#     opset_version=17, dynamic_axes={\"image\":{0:\"batch\"}}\n# )\n# print(\"Exported ONNX:\", f\"{OUT_DIR}/dog_emotion.onnx\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:11:31.821553Z","iopub.execute_input":"2025-08-23T11:11:31.822211Z","iopub.status.idle":"2025-08-23T11:19:25.696293Z","shell.execute_reply.started":"2025-08-23T11:11:31.822176Z","shell.execute_reply":"2025-08-23T11:19:25.695337Z"}},"outputs":[{"name":"stdout","text":"Using device: cuda\nClasses: ['angry', 'happy', 'relaxed', 'sad']\n\n== Stage 1: Linear Probe ==\n[Probe] 1/3 loss 1.1404 acc 0.5437\n[Probe] 2/3 loss 0.9127 acc 0.6716\n[Probe] 3/3 loss 0.8223 acc 0.6987\n\n== Stage 2: Main Training ==\n[Main] 1/20 | train loss 0.6006 acc 0.4931 f1 0.4304 | val loss 0.3893 acc 0.6288 f1 0.2098\n[Main] 2/20 | train loss 0.3378 acc 0.6891 f1 0.6702 | val loss 0.3598 acc 0.6637 f1 0.2140\n[Main] 3/20 | train loss 0.2912 acc 0.7344 f1 0.7143 | val loss 0.3558 acc 0.7362 f1 0.2331\n[Main] 4/20 | train loss 0.2453 acc 0.7716 f1 0.7597 | val loss 0.3019 acc 0.7638 f1 0.2370\n[Main] 5/20 | train loss 0.2335 acc 0.7700 f1 0.7584 | val loss 0.3191 acc 0.7600 f1 0.2356\n[Main] 6/20 | train loss 0.2093 acc 0.7903 f1 0.7780 | val loss 0.2880 acc 0.7588 f1 0.2369\n[Main] 7/20 | train loss 0.1985 acc 0.7984 f1 0.7862 | val loss 0.3134 acc 0.7750 f1 0.2412\n[Main] 8/20 | train loss 0.1908 acc 0.8097 f1 0.8002 | val loss 0.3266 acc 0.7750 f1 0.2395\n[Main] 9/20 | train loss 0.1831 acc 0.8081 f1 0.7958 | val loss 0.3066 acc 0.7562 f1 0.2355\n[Main] 10/20 | train loss 0.1635 acc 0.8284 f1 0.8197 | val loss 0.3230 acc 0.7662 f1 0.2385\n[Main] 11/20 | train loss 0.1597 acc 0.8347 f1 0.8219 | val loss 0.3150 acc 0.7638 f1 0.2369\n[Main] 12/20 | train loss 0.1734 acc 0.8281 f1 0.8160 | val loss 0.2661 acc 0.7950 f1 0.2409\n[Main] 13/20 | train loss 0.1557 acc 0.8275 f1 0.8178 | val loss 0.3330 acc 0.7312 f1 0.2278\n[Main] 14/20 | train loss 0.1542 acc 0.8375 f1 0.8289 | val loss 0.3173 acc 0.7688 f1 0.2344\n[Main] 15/20 | train loss 0.1518 acc 0.8334 f1 0.8247 | val loss 0.3704 acc 0.7700 f1 0.2368\n[Main] 16/20 | train loss 0.1577 acc 0.8447 f1 0.8362 | val loss 0.3287 acc 0.7700 f1 0.2365\n[Main] 17/20 | train loss 0.1415 acc 0.8538 f1 0.8443 | val loss 0.2942 acc 0.7700 f1 0.2373\n[Main] 18/20 | train loss 0.1379 acc 0.8547 f1 0.8447 | val loss 0.2906 acc 0.7987 f1 0.2424\n[Main] 19/20 | train loss 0.1353 acc 0.8578 f1 0.8508 | val loss 0.3153 acc 0.7887 f1 0.2426\n[Main] 20/20 | train loss 0.1256 acc 0.8644 f1 0.8570 | val loss 0.3535 acc 0.7812 f1 0.2395\nDone. Saved: /kaggle/working/best_model.pt\nClasses saved to: /kaggle/working/classes.json\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_36/369580668.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    260\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    261\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mcollections\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mdefaultdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 262\u001b[0;31m \u001b[0mckpt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{OUT_DIR}/best_model.pt\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_permutation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"cuda\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcuda\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"cpu\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    263\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mckpt\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"model\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0meval\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    264\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, weights_only, mmap, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1460\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mweights_only\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1461\u001b[0m                     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1462\u001b[0;31m                         return _load(\n\u001b[0m\u001b[1;32m   1463\u001b[0m                             \u001b[0mopened_zipfile\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1464\u001b[0m                             \u001b[0mmap_location\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/torch/serialization.py\u001b[0m in \u001b[0;36m_load\u001b[0;34m(zip_file, map_location, pickle_module, pickle_file, overall_storage, **pickle_load_args)\u001b[0m\n\u001b[1;32m   1956\u001b[0m     \u001b[0mdata_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBytesIO\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip_file\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_record\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpickle_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1957\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1958\u001b[0;31m     \u001b[0munpickler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUnpicklerWrapper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mpickle_load_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1959\u001b[0m     \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpersistent_load\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpersistent_load\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1960\u001b[0m     \u001b[0;31m# Needed for tensors where storage device and rebuild tensor device are\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mTypeError\u001b[0m: Unpickler.__init__() got an unexpected keyword argument 'map_permutation'"],"ename":"TypeError","evalue":"Unpickler.__init__() got an unexpected keyword argument 'map_permutation'","output_type":"error"}],"execution_count":2},{"cell_type":"code","source":"import os, numpy as np\nfrom collections import defaultdict\n\nckpt_path = f\"{OUT_DIR}/best_model.pt\"\nassert os.path.exists(ckpt_path), f\"Checkpoint not found: {ckpt_path}\"\n\n# 모델 클래스/구조(Backbone, PriorGuidedEmotionModel 등)가 이미 같은 셀/위에서 정의돼 있어야 합니다.\nckpt = torch.load(ckpt_path, map_location=(\"cuda\" if torch.cuda.is_available() else \"cpu\"))\nclasses = ckpt.get(\"classes\", [\"angry\",\"happy\",\"relaxed\",\"sad\"])\nnum_classes = len(classes)\n\n# 기존에 학습에 썼던 model 객체가 있다면 재사용:\n# model.load_state_dict(ckpt[\"model\"]); model.eval()\n\n# 만약 커널 재시작 등으로 model이 없다면, 동일 구조로 재생성 후 로드:\ntry:\n    model\nexcept NameError:\n    cfg = HeadConfig(num_classes=num_classes, d_model=512, n_heads=8, n_layers=3, lambda_prior=0.5, drop=0.1)\n    backbone = DinoResNet50Backbone().to(device)\n    model = PriorGuidedEmotionModel(backbone=backbone, cfg=cfg).to(device)\n\nmodel.load_state_dict(ckpt[\"model\"], strict=True)\nmodel.eval()\n\n# 혼동행렬 계산\ncm = np.zeros((num_classes, num_classes), dtype=int)\nper_cls = defaultdict(lambda: {\"tp\":0,\"fp\":0,\"fn\":0})\n\nwith torch.no_grad():\n    for imgs, labels in val_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        logits, _, _, _ = model(imgs, priors=None, return_attn=False)\n        preds = logits.argmax(-1)\n        for t,p in zip(labels.tolist(), preds.tolist()):\n            cm[t,p] += 1\n            if t==p: per_cls[t][\"tp\"]+=1\n            else: per_cls[p][\"fp\"]+=1; per_cls[t][\"fn\"]+=1\n\nprint(\"Confusion Matrix (rows=GT, cols=Pred):\\n\", cm)\nfor c,name in enumerate(classes):\n    tp,fp,fn = per_cls[c][\"tp\"], per_cls[c][\"fp\"], per_cls[c][\"fn\"]\n    prec = tp/(tp+fp+1e-8); rec = tp/(tp+fn+1e-8)\n    f1 = 0.0 if (prec+rec)==0 else 2*prec*rec/(prec+rec)\n    print(f\"{name:>8s} | P {prec:.3f} R {rec:.3f} F1 {f1:.3f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:21:05.944510Z","iopub.execute_input":"2025-08-23T11:21:05.944850Z","iopub.status.idle":"2025-08-23T11:21:08.845282Z","shell.execute_reply.started":"2025-08-23T11:21:05.944783Z","shell.execute_reply":"2025-08-23T11:21:08.844502Z"}},"outputs":[{"name":"stdout","text":"Confusion Matrix (rows=GT, cols=Pred):\n [[159  27   4  10]\n [  7 179  10   4]\n [ 18  19 147  16]\n [ 16   8  30 146]]\n   angry | P 0.795 R 0.795 F1 0.795\n   happy | P 0.768 R 0.895 F1 0.827\n relaxed | P 0.770 R 0.735 F1 0.752\n     sad | P 0.830 R 0.730 F1 0.777\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"# == Load best (baseline) ==\nckpt = torch.load(\"/kaggle/working/best_model.pt\", map_location=\"cuda\" if torch.cuda.is_available() else \"cpu\")\nclasses = ckpt[\"classes\"]\nnum_classes = len(classes)\n\n# model, backbone, cfg는 이전 셀에서 정의된 클래스를 그대로 사용\ncfg = HeadConfig(num_classes=num_classes, d_model=512, n_heads=8, n_layers=3, lambda_prior=0.5, drop=0.1)\nbackbone = DinoResNet50Backbone().to(device)\nmodel_prior = PriorGuidedEmotionModel(backbone=backbone, cfg=cfg).to(device)\nmodel_prior.load_state_dict(ckpt[\"model\"], strict=False)\n\n# layer4만 학습 가능하도록 해제 (나머지는 고정)\nfor p in backbone.resnet.layer4.parameters():\n    p.requires_grad = True\nfor name, m in backbone.resnet.named_children():\n    if name in [\"conv1\",\"bn1\",\"layer1\",\"layer2\",\"layer3\"]:\n        for p in m.parameters():\n            p.requires_grad = False\n\n# == 간단 saliency prior: conv5 합(ReLU) → [0,1] 정규화 ==\n@torch.no_grad()\ndef compute_prior_simple(images):\n    feat,_ = backbone(images)          # (B,2048,7,7)\n    cam = torch.relu(feat.sum(1))      # (B,7,7)\n    B = cam.size(0)\n    cam = cam.view(B,-1)\n    cam = (cam - cam.min(1,keepdim=True).values) / (cam.max(1,keepdim=True).values - cam.min(1,keepdim=True).values + 1e-6)\n    return cam.view(-1,7,7)            # (B,7,7)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:21:49.840403Z","iopub.execute_input":"2025-08-23T11:21:49.840976Z","iopub.status.idle":"2025-08-23T11:21:50.533081Z","shell.execute_reply.started":"2025-08-23T11:21:49.840950Z","shell.execute_reply":"2025-08-23T11:21:50.532481Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"EPOCHS_FT = 8\nLR_FT     = 1e-4\nWD_FT     = 0.05\ncriterion = nn.CrossEntropyLoss()   # 안정적 수렴용; 희망 시 focal로 대체 가능\noptimizer = torch.optim.AdamW(filter(lambda p: p.requires_grad, model_prior.parameters()),\n                              lr=LR_FT, weight_decay=WD_FT)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=EPOCHS_FT)\n\nbest_f1_ft = -1.0\nprint(\"\\n== FT with priors (layer4) ==\")\nfor ep in range(1, EPOCHS_FT+1):\n    # train\n    model_prior.train(); tl=ta=tf=0\n    for imgs, labels in train_loader:\n        imgs, labels = imgs.to(device), labels.to(device)\n        pri = compute_prior_simple(imgs).to(device)    # (B,7,7)\n        logits, _, _, _ = model_prior(imgs, priors=pri, return_attn=False)\n        loss = criterion(logits, labels)\n        optimizer.zero_grad(); loss.backward(); optimizer.step()\n\n        tl += loss.item()*imgs.size(0)\n        ta += (logits.argmax(-1)==labels).float().sum().item()\n        tf += macro_f1(logits, labels, num_classes)*imgs.size(0)\n\n    scheduler.step()\n\n    # val\n    model_prior.eval(); vl=va=vf=0\n    with torch.no_grad():\n        for imgs, labels in val_loader:\n            imgs, labels = imgs.to(device), labels.to(device)\n            pri = compute_prior_simple(imgs).to(device)\n            logits, _, _, _ = model_prior(imgs, priors=pri, return_attn=False)\n            loss = criterion(logits, labels)\n            vl += loss.item()*imgs.size(0)\n            va += (logits.argmax(-1)==labels).float().sum().item()\n            vf += macro_f1(logits, labels, num_classes)*imgs.size(0)\n\n    Ntr, Nval = len(train_loader.dataset), len(val_loader.dataset)\n    tr_loss, tr_acc, tr_f1 = tl/Ntr, ta/Ntr, tf/Ntr\n    val_loss, val_acc, val_f1 = vl/Nval, va/Nval, vf/Nval\n    print(f\"[FT] {ep}/{EPOCHS_FT} | tr {tr_loss:.4f} acc {tr_acc:.4f} f1 {tr_f1:.4f} \"\n          f\"| val {val_loss:.4f} acc {val_acc:.4f} f1 {val_f1:.4f}\")\n\n    if val_f1 > best_f1_ft:\n        best_f1_ft = val_f1\n        torch.save({\"model\":model_prior.state_dict(),\"cfg\":cfg.__dict__,\"classes\":classes},\n                   \"/kaggle/working/best_model_prior.pt\")\n\nprint(\"Saved:\", \"/kaggle/working/best_model_prior.pt\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:22:04.068650Z","iopub.execute_input":"2025-08-23T11:22:04.068955Z","iopub.status.idle":"2025-08-23T11:25:09.962435Z","shell.execute_reply.started":"2025-08-23T11:22:04.068932Z","shell.execute_reply":"2025-08-23T11:25:09.961668Z"}},"outputs":[{"name":"stdout","text":"\n== FT with priors (layer4) ==\n[FT] 1/8 | tr 0.3298 acc 0.8816 f1 0.8704 | val 0.6179 acc 0.7963 f1 0.2420\n[FT] 2/8 | tr 0.2246 acc 0.9184 f1 0.9084 | val 0.5688 acc 0.8387 f1 0.2521\n[FT] 3/8 | tr 0.1746 acc 0.9431 f1 0.9336 | val 0.5480 acc 0.8413 f1 0.2507\n[FT] 4/8 | tr 0.1325 acc 0.9541 f1 0.9507 | val 0.5925 acc 0.8400 f1 0.2515\n[FT] 5/8 | tr 0.1043 acc 0.9697 f1 0.9682 | val 0.6062 acc 0.8313 f1 0.2498\n[FT] 6/8 | tr 0.0763 acc 0.9747 f1 0.9732 | val 0.6149 acc 0.8462 f1 0.2523\n[FT] 7/8 | tr 0.0695 acc 0.9778 f1 0.9752 | val 0.6489 acc 0.8337 f1 0.2496\n[FT] 8/8 | tr 0.0569 acc 0.9800 f1 0.9788 | val 0.6634 acc 0.8375 f1 0.2494\nSaved: /kaggle/working/best_model_prior.pt\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# == Grad-CAM prior (cls score의 grad로 CAM 생성) ==\ndef compute_prior_gradcam(images, target_layer=None):\n    if target_layer is None:\n        target_layer = backbone.resnet.layer4  # conv5\n    acts = []\n    grads = []\n\n    def fwd_hook(module, inp, out): acts.append(out)       # (B,2048,7,7)\n    def bwd_hook(module, ginp, gout): grads.append(gout[0])# (B,2048,7,7)\n\n    h1 = target_layer.register_forward_hook(fwd_hook)\n    h2 = target_layer.register_full_backward_hook(bwd_hook)\n\n    model_prior.eval()\n    images = images.requires_grad_(True)\n    logits,_,_,_ = model_prior(images, priors=None, return_attn=False)\n    # 최대 점수 클래스 기준 CAM\n    top = logits.argmax(dim=1)\n    loss = logits.gather(1, top.unsqueeze(1)).sum()\n    model_prior.zero_grad(); loss.backward()\n\n    feat = acts[0]                      # (B,C,H,W)\n    grad = grads[0]                     # (B,C,H,W)\n    w = grad.mean(dim=(2,3), keepdim=True)\n    cam = torch.relu((w*feat).sum(1))   # (B,H,W)\n    # normalize\n    B = cam.size(0); cam = cam.view(B,-1)\n    cam = (cam - cam.min(1,keepdim=True).values) / (cam.max(1,keepdim=True).values - cam.min(1,keepdim=True).values + 1e-6)\n    cam = cam.view(-1, feat.size(2), feat.size(3))\n\n    h1.remove(); h2.remove()\n    return cam.detach()\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"from PIL import Image\ninfer_tf = transforms.Compose([\n    transforms.Resize(int(IMG_SIZE*1.15)),\n    transforms.CenterCrop(IMG_SIZE),\n    transforms.ToTensor(),\n    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n])\n\n# 최종 모델 로드 (prior FT 버전 권장)\nck = torch.load(\"/kaggle/working/best_model_prior.pt\", map_location=device)\nmodel_prior.load_state_dict(ck[\"model\"]); model_prior.eval()\n\ndef predict(path, use_prior=True):\n    img = infer_tf(Image.open(path).convert(\"RGB\")).unsqueeze(0).to(device)\n    pri = compute_prior_simple(img).to(device) if use_prior else None\n    with torch.no_grad():\n        logits,_,_,_ = model_prior(img, priors=pri, return_attn=False)\n        probs = logits.softmax(-1).squeeze().tolist()\n    return dict(zip(classes, [round(p,3) for p in probs]))\n\nprint(predict(\"/kaggle/input/dog-emotion/Dog Emotion/happy/.jpg\", use_prior=True))\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:25:31.185949Z","iopub.execute_input":"2025-08-23T11:25:31.186741Z","iopub.status.idle":"2025-08-23T11:25:31.361109Z","shell.execute_reply.started":"2025-08-23T11:25:31.186712Z","shell.execute_reply":"2025-08-23T11:25:31.360556Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"dummy = torch.randn(1,3,IMG_SIZE,IMG_SIZE, device=device)\ntorch.onnx.export(\n    model_prior, (dummy, None),  # prior 없이 고정 내보내기\n    \"/kaggle/working/dog_emotion_prior.onnx\",\n    input_names=[\"image\",\"prior\"], output_names=[\"logits\",\"val\",\"ar\"],\n    opset_version=17, dynamic_axes={\"image\":{0:\"batch\"}}\n)\nprint(\"Exported:\", \"/kaggle/working/dog_emotion_prior.onnx\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-23T11:31:30.718149Z","iopub.execute_input":"2025-08-23T11:31:30.718693Z","iopub.status.idle":"2025-08-23T11:31:33.325482Z","shell.execute_reply.started":"2025-08-23T11:31:30.718667Z","shell.execute_reply":"2025-08-23T11:31:33.324709Z"}},"outputs":[{"name":"stdout","text":"Exported: /kaggle/working/dog_emotion_prior.onnx\n","output_type":"stream"}],"execution_count":8}]}